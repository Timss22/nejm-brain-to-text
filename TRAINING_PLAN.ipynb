{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1def15a7",
   "metadata": {},
   "source": [
    "# Brain-to-Text Competition: Training Plan & Platform Options\n",
    "\n",
    "## üìã Project Summary\n",
    "\n",
    "**Goal:** Train and evaluate a brain-to-text model for the [Kaggle Brain-to-Text '25 Competition](https://www.kaggle.com/competitions/brain-to-text-25)\n",
    "\n",
    "**Objective:** Decode neural signals from the speech motor cortex into text using a two-stage pipeline:\n",
    "1. **RNN Model** (GRU-based): Predicts phonemes from neural data (512 features from 256 electrodes)\n",
    "2. **Language Model** (Ngram + OPT 6.7b): Converts phoneme sequences to text predictions\n",
    "\n",
    "**Evaluation Metric:** Word Error Rate (WER) - lower is better\n",
    "\n",
    "**Output Format:** CSV file with `id` and `text` columns for Kaggle submission\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task Breakdown\n",
    "\n",
    "### Phase 1: Environment Setup\n",
    "1. **Data Preparation**\n",
    "   - Download datasets from Dryad (~10GB+)\n",
    "   - Verify data directory structure\n",
    "   - Unzip neural data files\n",
    "\n",
    "2. **Environment Configuration**\n",
    "   - Set up conda environment for model training (`b2txt25`)\n",
    "   - Set up conda environment for language model (`b2txt25_lm`)\n",
    "   - Install system dependencies (Redis, CMake, gcc)\n",
    "   - Verify GPU availability and CUDA compatibility\n",
    "\n",
    "### Phase 2: Model Training\n",
    "3. **Baseline RNN Training**\n",
    "   - Configure training hyperparameters (`rnn_args.yaml`)\n",
    "   - Train GRU decoder model (120,000 batches, ~3.5 hours on RTX 4090)\n",
    "   - Monitor validation Phoneme Error Rate (PER)\n",
    "   - Save best checkpoint based on validation metrics\n",
    "   - Target: Achieve ~10.1% aggregate PER on validation set\n",
    "\n",
    "4. **Model Evaluation (Validation)**\n",
    "   - Load trained model checkpoint\n",
    "   - Run inference on validation set to get phoneme logits\n",
    "   - Pass logits through language model to get word predictions\n",
    "   - Calculate WER on validation set\n",
    "   - Generate submission CSV for validation split\n",
    "\n",
    "### Phase 3: Submission Generation\n",
    "5. **Test Set Inference**\n",
    "   - Run inference on test set (no ground truth available)\n",
    "   - Generate final submission CSV with predictions\n",
    "   - Format: `id,text` columns\n",
    "   - Submit to Kaggle competition\n",
    "\n",
    "### Phase 4: Model Improvement (Optional)\n",
    "6. **Hyperparameter Tuning**\n",
    "   - Experiment with different model architectures\n",
    "   - Adjust learning rates, dropout, batch sizes\n",
    "   - Try different data augmentation strategies\n",
    "   - Experiment with different language models (1gram, 3gram, 5gram)\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Detailed Implementation Plan\n",
    "\n",
    "### Step 1: Data Setup\n",
    "```bash\n",
    "# Navigate to project root\n",
    "cd /Users/tim/Documents/timo/semester7/DataMining/KaggleCompetition/nejm-brain-to-text\n",
    "\n",
    "# Activate conda environment\n",
    "conda activate b2txt25\n",
    "\n",
    "# Download data from Dryad\n",
    "python download_data.py\n",
    "\n",
    "# Verify data structure:\n",
    "# data/\n",
    "# ‚îú‚îÄ‚îÄ t15_copyTask.pkl\n",
    "# ‚îú‚îÄ‚îÄ t15_personalUse.pkl\n",
    "# ‚îú‚îÄ‚îÄ hdf5_data_final/          # Unzipped from t15_copyTask_neuralData.zip\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ t15.2023.08.11/\n",
    "# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_train.hdf5\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ t15.2023.08.13/\n",
    "# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_train.hdf5\n",
    "# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_val.hdf5\n",
    "# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_test.hdf5\n",
    "# ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "# ‚îî‚îÄ‚îÄ t15_pretrained_rnn_baseline/  # Unzipped from t15_pretrained_rnn_baseline.zip\n",
    "#     ‚îú‚îÄ‚îÄ checkpoint/\n",
    "#     ‚îÇ   ‚îú‚îÄ‚îÄ args.yaml\n",
    "#     ‚îÇ   ‚îú‚îÄ‚îÄ best_checkpoint\n",
    "#     ‚îî‚îÄ‚îÄ training_log\n",
    "```\n",
    "\n",
    "### Step 2: Environment Setup\n",
    "\n",
    "#### For Model Training (`b2txt25`):\n",
    "```bash\n",
    "# From project root\n",
    "./setup.sh\n",
    "\n",
    "# Verify installation\n",
    "conda activate b2txt25\n",
    "python -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')\"\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.10\n",
    "- PyTorch with CUDA 12.6\n",
    "- Redis, NumPy, Pandas, h5py, etc. (see `setup.sh`)\n",
    "\n",
    "#### For Language Model (`b2txt25_lm`):\n",
    "```bash\n",
    "# From project root\n",
    "./setup_lm.sh\n",
    "\n",
    "# Verify installation\n",
    "conda activate b2txt25_lm\n",
    "python -c \"import torch; print(f'PyTorch: {torch.__version__}')\"\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.9\n",
    "- PyTorch 1.13.1 (older version for LM compatibility)\n",
    "- CMake >= 3.14\n",
    "- gcc >= 10.1\n",
    "\n",
    "#### System Dependencies:\n",
    "```bash\n",
    "# Install Redis (Ubuntu/Debian)\n",
    "sudo apt-get update\n",
    "sudo apt-get install redis-server build-essential cmake\n",
    "\n",
    "# Disable Redis auto-restart\n",
    "sudo systemctl disable redis-server\n",
    "```\n",
    "\n",
    "### Step 3: Training Configuration\n",
    "\n",
    "**File:** `model_training/rnn_args.yaml`\n",
    "\n",
    "**Key Parameters to Review:**\n",
    "- `gpu_number`: Set to available GPU (default: '1')\n",
    "- `num_training_batches`: Default 120,000 (~3.5 hours on RTX 4090)\n",
    "- `batch_size`: Default 64 (adjust based on GPU memory)\n",
    "- `lr_max`: Default 0.005 (learning rate)\n",
    "- `output_dir`: Where to save trained model\n",
    "- `checkpoint_dir`: Where to save checkpoints\n",
    "\n",
    "**Training Sessions:**\n",
    "- 45 sessions spanning 20 months\n",
    "- 10,948 sentences total\n",
    "- Training/validation split defined in `dataset_probability_val` array\n",
    "\n",
    "### Step 4: Model Training\n",
    "\n",
    "```bash\n",
    "cd model_training\n",
    "conda activate b2txt25\n",
    "\n",
    "# Train the model\n",
    "python train_model.py\n",
    "\n",
    "# Monitor training:\n",
    "# - Training logs saved to: trained_models/baseline_rnn/training_log\n",
    "# - Best checkpoint saved to: trained_models/baseline_rnn/checkpoint/best_checkpoint\n",
    "# - Validation metrics saved to: trained_models/baseline_rnn/checkpoint/val_metrics.pkl\n",
    "```\n",
    "\n",
    "**Expected Training Time:**\n",
    "- ~3.5 hours on RTX 4090\n",
    "- ~7-10 hours on RTX 3080\n",
    "- ~15-20 hours on RTX 3060\n",
    "- Much longer on CPU (not recommended)\n",
    "\n",
    "**Monitoring:**\n",
    "- Validation PER should decrease over time\n",
    "- Target: ~10.1% aggregate PER on validation set\n",
    "- Check training log for progress every 200 batches\n",
    "\n",
    "### Step 5: Model Evaluation\n",
    "\n",
    "#### Step 5a: Start Redis Server\n",
    "```bash\n",
    "# In a separate terminal\n",
    "redis-server\n",
    "\n",
    "# Keep this running during evaluation\n",
    "```\n",
    "\n",
    "#### Step 5b: Start Language Model\n",
    "```bash\n",
    "# In another separate terminal\n",
    "cd /path/to/nejm-brain-to-text\n",
    "conda activate b2txt25_lm\n",
    "\n",
    "# For 1gram model (lightweight, no grammatical structure)\n",
    "python language_model/language-model-standalone.py \\\n",
    "    --lm_path language_model/pretrained_language_models/openwebtext_1gram_lm_sil \\\n",
    "    --do_opt \\\n",
    "    --nbest 100 \\\n",
    "    --acoustic_scale 0.325 \\\n",
    "    --blank_penalty 90 \\\n",
    "    --alpha 0.55 \\\n",
    "    --redis_ip localhost \\\n",
    "    --gpu_number 0\n",
    "\n",
    "# For 3gram model (requires ~60GB RAM, better accuracy)\n",
    "python language_model/language-model-standalone.py \\\n",
    "    --lm_path language_model/pretrained_language_models/openwebtext_3gram_lm_sil \\\n",
    "    --do_opt \\\n",
    "    --nbest 100 \\\n",
    "    --acoustic_scale 0.325 \\\n",
    "    --blank_penalty 90 \\\n",
    "    --alpha 0.55 \\\n",
    "    --redis_ip localhost \\\n",
    "    --gpu_number 0\n",
    "\n",
    "# For 5gram model (requires ~300GB RAM, best accuracy)\n",
    "python language_model/language-model-standalone.py \\\n",
    "    --lm_path language_model/pretrained_language_models/openwebtext_5gram_lm_sil \\\n",
    "    --rescore \\\n",
    "    --do_opt \\\n",
    "    --nbest 100 \\\n",
    "    --acoustic_scale 0.325 \\\n",
    "    --blank_penalty 90 \\\n",
    "    --alpha 0.55 \\\n",
    "    --redis_ip localhost \\\n",
    "    --gpu_number 0\n",
    "```\n",
    "\n",
    "**Note:** First run will download OPT-6.7b from HuggingFace (~13GB)\n",
    "\n",
    "#### Step 5c: Run Evaluation\n",
    "```bash\n",
    "# In main terminal\n",
    "cd model_training\n",
    "conda activate b2txt25\n",
    "\n",
    "# Evaluate on validation set (for testing)\n",
    "python evaluate_model.py \\\n",
    "    --model_path trained_models/baseline_rnn \\\n",
    "    --data_dir ../data/hdf5_data_final \\\n",
    "    --eval_type val \\\n",
    "    --gpu_number 1\n",
    "\n",
    "# Evaluate on test set (for submission)\n",
    "python evaluate_model.py \\\n",
    "    --model_path trained_models/baseline_rnn \\\n",
    "    --data_dir ../data/hdf5_data_final \\\n",
    "    --eval_type test \\\n",
    "    --gpu_number 1\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "- CSV file: `baseline_rnn_{eval_type}_predicted_sentences_YYYYMMDD_HHMMSS.csv`\n",
    "- Contains `id` and `text` columns ready for Kaggle submission\n",
    "- For validation set, also prints WER metrics\n",
    "\n",
    "#### Step 5d: Shutdown\n",
    "```bash\n",
    "# When done, shutdown Redis\n",
    "redis-cli shutdown\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üñ•Ô∏è Platform Options for Training\n",
    "\n",
    "### Option 1: Local Machine (macOS - Current Setup)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ No setup required, already have the code\n",
    "- ‚úÖ Full control over environment\n",
    "- ‚úÖ No internet dependency during training\n",
    "- ‚úÖ Easy to iterate and debug\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå **No GPU support on macOS** (Apple Silicon uses Metal, not CUDA)\n",
    "- ‚ùå Training will be extremely slow on CPU (days/weeks)\n",
    "- ‚ùå Language model inference requires GPU with 12.4GB+ VRAM\n",
    "- ‚ùå Large language models (3gram/5gram) require massive RAM\n",
    "\n",
    "**Verdict:** ‚ùå **NOT RECOMMENDED** for training. Only use for code development and testing.\n",
    "\n",
    "**Recommendation:** Use this setup for code development, then train on a GPU-enabled platform.\n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: WSL2 (Windows Subsystem for Linux) with NVIDIA GPU\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Native Linux environment (Ubuntu 22.04 recommended)\n",
    "- ‚úÖ Direct GPU access if NVIDIA GPU is available\n",
    "- ‚úÖ Can run on Windows machine\n",
    "- ‚úÖ Full control over environment\n",
    "- ‚úÖ No cloud costs\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Requires Windows 11 with WSL2\n",
    "- ‚ùå Requires NVIDIA GPU with CUDA support\n",
    "- ‚ùå Requires NVIDIA drivers for WSL2\n",
    "- ‚ùå Setup complexity (GPU passthrough)\n",
    "- ‚ùå Limited by local hardware resources\n",
    "\n",
    "**Setup Requirements:**\n",
    "1. Windows 11 with WSL2 installed\n",
    "2. NVIDIA GPU with CUDA support (RTX series recommended)\n",
    "3. NVIDIA drivers for WSL2\n",
    "4. Ubuntu 22.04 distribution in WSL2\n",
    "\n",
    "**Estimated Cost:** Free (uses existing hardware)\n",
    "\n",
    "**Best For:** Users with Windows machine + NVIDIA GPU\n",
    "\n",
    "**Setup Steps:**\n",
    "```bash\n",
    "# Install WSL2 with Ubuntu 22.04\n",
    "wsl --install -d Ubuntu-22.04\n",
    "\n",
    "# Install NVIDIA drivers for WSL2\n",
    "# Download from: https://www.nvidia.com/Download/index.aspx\n",
    "\n",
    "# Inside WSL2, install CUDA toolkit\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\n",
    "sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda-repo-wsl-ubuntu-12-6-local_12.6.0-1_amd64.deb\n",
    "sudo dpkg -i cuda-repo-wsl-ubuntu-12-6-local_12.6.0-1_amd64.deb\n",
    "sudo cp /var/cuda-repo-wsl-ubuntu-12-6-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install cuda\n",
    "\n",
    "# Verify CUDA\n",
    "nvidia-smi\n",
    "nvcc --version\n",
    "\n",
    "# Clone repository and setup\n",
    "git clone <repo-url>\n",
    "cd nejm-brain-to-text\n",
    "./setup.sh\n",
    "./setup_lm.sh\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Option 3: Google Colab (Free Tier)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Free GPU access (T4, 16GB VRAM)\n",
    "- ‚úÖ Pre-configured environment\n",
    "- ‚úÖ No local setup required\n",
    "- ‚úÖ Easy to share and collaborate\n",
    "- ‚úÖ Jupyter notebook interface\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå **Limited runtime** (12 hours max, then disconnects)\n",
    "- ‚ùå **Training may not complete** in one session (3.5+ hours needed)\n",
    "- ‚ùå Unstable connection (can disconnect)\n",
    "- ‚ùå Limited storage (need to upload/download data)\n",
    "- ‚ùå Can't run Redis server easily\n",
    "- ‚ùå Difficult to run language model pipeline\n",
    "- ‚ùå No guarantee of GPU availability\n",
    "\n",
    "**Estimated Cost:** Free (with limitations)\n",
    "\n",
    "**Best For:** Quick experiments, testing code, prototyping\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Upload project to Google Drive or GitHub\n",
    "2. Mount Google Drive in Colab\n",
    "3. Install dependencies in Colab notebook\n",
    "4. Run training (may need multiple sessions)\n",
    "\n",
    "**Limitations:**\n",
    "- Training time: ~3.5 hours (but session limits: 12 hours max)\n",
    "- May need to save checkpoints and resume\n",
    "- Language model evaluation is complex in Colab\n",
    "\n",
    "**Verdict:** ‚ö†Ô∏è **POSSIBLE BUT CHALLENGING** - Good for prototyping, not ideal for full pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### Option 4: Google Colab Pro ($10/month)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ More reliable GPU access (T4, A100 options)\n",
    "- ‚úÖ Longer runtime sessions\n",
    "- ‚úÖ Better performance\n",
    "- ‚úÖ Priority access to GPUs\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Still has session limits\n",
    "- ‚ùå Monthly subscription cost\n",
    "- ‚ùå Storage limitations\n",
    "- ‚ùå Complex setup for full pipeline\n",
    "\n",
    "**Estimated Cost:** $10/month\n",
    "\n",
    "**Best For:** Users who want better Colab experience\n",
    "\n",
    "**Verdict:** ‚ö†Ô∏è **BETTER THAN FREE TIER** but still has limitations\n",
    "\n",
    "---\n",
    "\n",
    "### Option 5: Kaggle Notebooks (Free)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Free GPU access (P100, 16GB VRAM)\n",
    "- ‚úÖ 30 hours/week GPU time limit\n",
    "- ‚úÖ Pre-configured environment\n",
    "- ‚úÖ Competition-specific platform\n",
    "- ‚úÖ Easy data access\n",
    "- ‚úÖ Can run for ~9 hours per session\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Limited to 30 hours/week GPU time\n",
    "- ‚ùå Session limits (~9 hours max)\n",
    "- ‚ùå Storage limitations\n",
    "- ‚ùå Complex setup for Redis + language model\n",
    "- ‚ùå Internet access restrictions\n",
    "\n",
    "**Estimated Cost:** Free\n",
    "\n",
    "**Best For:** Competition participants, quick experiments\n",
    "\n",
    "**Setup Approach:**\n",
    "1. Upload project as Kaggle dataset\n",
    "2. Create new notebook with GPU enabled\n",
    "3. Install dependencies\n",
    "4. Run training (may need to save/resume)\n",
    "\n",
    "**Verdict:** ‚ö†Ô∏è **GOOD FOR COMPETITION** but may need multiple sessions for full training\n",
    "\n",
    "---\n",
    "\n",
    "### Option 6: AWS EC2 (Recommended)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Full control over environment\n",
    "- ‚úÖ Choose GPU instance (g4dn, p3, p4d)\n",
    "- ‚úÖ Can run complete pipeline\n",
    "- ‚úÖ Persistent storage (EBS)\n",
    "- ‚úÖ Can run 24/7 if needed\n",
    "- ‚úÖ Professional setup\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Costs money ($0.50-$10+/hour depending on instance)\n",
    "- ‚ùå Requires AWS account setup\n",
    "- ‚ùå Need to manage instance lifecycle\n",
    "- ‚ùå More complex initial setup\n",
    "\n",
    "**Estimated Cost:** \n",
    "- **g4dn.xlarge** (T4, 16GB): ~$0.50/hour = **~$1.75 per training run**\n",
    "- **p3.2xlarge** (V100, 16GB): ~$3.06/hour = **~$10.71 per training run**\n",
    "- **p4d.24xlarge** (A100, 40GB): ~$32.77/hour = **~$114.70 per training run**\n",
    "\n",
    "**Best For:** Serious training, production workloads\n",
    "\n",
    "**Recommended Instance:** `g4dn.xlarge` (T4 GPU, 16GB VRAM, sufficient for training)\n",
    "\n",
    "**Setup Steps:**\n",
    "```bash\n",
    "# 1. Launch EC2 instance\n",
    "# - AMI: Ubuntu 22.04 LTS\n",
    "# - Instance: g4dn.xlarge (or larger)\n",
    "# - Storage: 100GB+ (for data and models)\n",
    "# - Security Group: Allow SSH (port 22)\n",
    "\n",
    "# 2. Connect via SSH\n",
    "ssh -i your-key.pem ubuntu@your-instance-ip\n",
    "\n",
    "# 3. Install NVIDIA drivers and CUDA\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y build-essential\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda-repo-ubuntu2204-12-6-local_12.6.0-1_amd64.deb\n",
    "sudo dpkg -i cuda-repo-ubuntu2204-12-6-local_12.6.0-1_amd64.deb\n",
    "sudo cp /var/cuda-repo-ubuntu2204-12-6-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install cuda-toolkit-12-6\n",
    "sudo apt-get -y install nvidia-driver-550\n",
    "\n",
    "# 4. Install Redis, CMake, GCC\n",
    "sudo apt-get install -y redis-server build-essential cmake\n",
    "sudo systemctl disable redis-server\n",
    "\n",
    "# 5. Install Miniconda\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh -b\n",
    "source ~/miniconda3/bin/activate\n",
    "\n",
    "# 6. Clone repository and setup\n",
    "git clone <repo-url>  # or upload via SCP\n",
    "cd nejm-brain-to-text\n",
    "./setup.sh\n",
    "./setup_lm.sh\n",
    "\n",
    "# 7. Download data\n",
    "conda activate b2txt25\n",
    "python download_data.py\n",
    "\n",
    "# 8. Train model (use screen or tmux for long-running jobs)\n",
    "screen -S training\n",
    "conda activate b2txt25\n",
    "cd model_training\n",
    "python train_model.py\n",
    "# Press Ctrl+A then D to detach\n",
    "\n",
    "# 9. Monitor training\n",
    "screen -r training\n",
    "\n",
    "# 10. Download results when done\n",
    "# Use SCP to download trained models\n",
    "scp -i your-key.pem -r ubuntu@your-instance-ip:~/nejm-brain-to-text/trained_models ./\n",
    "```\n",
    "\n",
    "**Cost Optimization Tips:**\n",
    "- Use Spot Instances for 70% cost savings (but can be interrupted)\n",
    "- Stop instance when not training\n",
    "- Use smaller instance for evaluation only\n",
    "- Consider Reserved Instances for long-term use\n",
    "\n",
    "**Verdict:** ‚úÖ **HIGHLY RECOMMENDED** for serious training\n",
    "\n",
    "---\n",
    "\n",
    "### Option 7: Google Cloud Platform (GCP)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Similar to AWS, full control\n",
    "- ‚úÖ Good GPU options\n",
    "- ‚úÖ $300 free credits for new users\n",
    "- ‚úÖ Persistent storage\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Costs money after free credits\n",
    "- ‚ùå More complex setup\n",
    "- ‚ùå Need GCP account\n",
    "\n",
    "**Estimated Cost:**\n",
    "- **n1-standard-4 + T4 GPU**: ~$0.35/hour = **~$1.23 per training run**\n",
    "- **n1-standard-8 + V100**: ~$2.50/hour = **~$8.75 per training run**\n",
    "\n",
    "**Best For:** Users with GCP credits or preference\n",
    "\n",
    "**Setup:** Similar to AWS, but using GCP Compute Engine\n",
    "\n",
    "**Verdict:** ‚úÖ **GOOD ALTERNATIVE TO AWS**\n",
    "\n",
    "---\n",
    "\n",
    "### Option 8: Azure ML / Azure Compute\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Managed ML platform\n",
    "- ‚úÖ Good GPU options\n",
    "- ‚úÖ $200 free credits for new users\n",
    "- ‚úÖ Integration with ML tools\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Costs money\n",
    "- ‚ùå More complex setup\n",
    "- ‚ùå Need Azure account\n",
    "\n",
    "**Estimated Cost:** Similar to AWS/GCP\n",
    "\n",
    "**Verdict:** ‚úÖ **GOOD OPTION** if you prefer Azure ecosystem\n",
    "\n",
    "---\n",
    "\n",
    "### Option 9: Lambda Labs / Vast.ai / RunPod (GPU Rental)\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Cheaper than AWS/GCP (often 50-70% less)\n",
    "- ‚úÖ Pay per hour\n",
    "- ‚úÖ Good GPU selection\n",
    "- ‚úÖ Simple setup\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Less established providers\n",
    "- ‚ùå May have less reliability\n",
    "- ‚ùå Need to trust third-party\n",
    "\n",
    "**Estimated Cost:**\n",
    "- **RTX 3090 (24GB)**: ~$0.35/hour = **~$1.23 per training run**\n",
    "- **A100 (40GB)**: ~$1.10/hour = **~$3.85 per training run**\n",
    "\n",
    "**Best For:** Cost-conscious users\n",
    "\n",
    "**Verdict:** ‚úÖ **COST-EFFECTIVE OPTION**\n",
    "\n",
    "---\n",
    "\n",
    "### Option 10: University/Research Compute Cluster\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Often free for students/researchers\n",
    "- ‚úÖ High-performance GPUs\n",
    "- ‚úÖ Professional infrastructure\n",
    "- ‚úÖ Support available\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå May require approval/access\n",
    "- ‚ùå May have usage limits\n",
    "- ‚ùå Less control\n",
    "- ‚ùå May have queue waiting times\n",
    "\n",
    "**Best For:** Students with access to university resources\n",
    "\n",
    "**Verdict:** ‚úÖ **BEST IF AVAILABLE**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Platform Recommendation Summary\n",
    "\n",
    "### For Training (Ranked by Preference):\n",
    "\n",
    "1. **ü•á University Compute Cluster** (if available)\n",
    "   - Free, high-performance, professional setup\n",
    "\n",
    "2. **ü•à AWS EC2 (g4dn.xlarge)** \n",
    "   - ~$1.75 per training run\n",
    "   - Full control, reliable, professional\n",
    "\n",
    "3. **ü•â Lambda Labs / Vast.ai**\n",
    "   - ~$1.23 per training run (RTX 3090)\n",
    "   - Cost-effective, good performance\n",
    "\n",
    "4. **Kaggle Notebooks**\n",
    "   - Free, but limited to 30 hours/week\n",
    "   - Good for competition, may need multiple sessions\n",
    "\n",
    "5. **Google Colab Pro**\n",
    "   - $10/month, but still has limitations\n",
    "   - Good for prototyping\n",
    "\n",
    "6. **WSL2 (if you have NVIDIA GPU)**\n",
    "   - Free, but requires Windows + NVIDIA GPU\n",
    "   - Good if hardware is available\n",
    "\n",
    "7. **Local macOS**\n",
    "   - ‚ùå Not recommended (no CUDA support)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resource Requirements Summary\n",
    "\n",
    "### Minimum Requirements:\n",
    "- **GPU**: NVIDIA GPU with CUDA support (8GB+ VRAM minimum, 16GB+ recommended)\n",
    "- **RAM**: 16GB minimum (60GB+ for 3gram LM, 300GB+ for 5gram LM)\n",
    "- **Storage**: 50GB+ for data and models\n",
    "- **OS**: Ubuntu 22.04 (recommended) or Linux equivalent\n",
    "- **Training Time**: ~3.5 hours on RTX 4090, longer on slower GPUs\n",
    "\n",
    "### Recommended Setup:\n",
    "- **GPU**: RTX 3090/4090, V100, or A100 (16GB+ VRAM)\n",
    "- **RAM**: 32GB+ (64GB+ for 3gram LM)\n",
    "- **Storage**: 100GB+ SSD\n",
    "- **OS**: Ubuntu 22.04 LTS\n",
    "- **Network**: Stable connection for data download\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Considerations\n",
    "\n",
    "1. **Training Interruptions**: \n",
    "   - Save checkpoints regularly (configured in `rnn_args.yaml`)\n",
    "   - Use `screen` or `tmux` for long-running jobs\n",
    "   - Consider resumable training if interrupted\n",
    "\n",
    "2. **Data Storage**:\n",
    "   - Data is ~10GB+ compressed\n",
    "   - Unzipped data is larger\n",
    "   - Trained models are several GB\n",
    "   - Plan for sufficient storage\n",
    "\n",
    "3. **Language Model Requirements**:\n",
    "   - OPT 6.7b requires 12.4GB+ VRAM\n",
    "   - 3gram LM requires ~60GB RAM\n",
    "   - 5gram LM requires ~300GB RAM\n",
    "   - May need to use smaller models or upgrade hardware\n",
    "\n",
    "4. **Redis Server**:\n",
    "   - Required for language model inference\n",
    "   - Must run during evaluation\n",
    "   - Can run on same machine or separate instance\n",
    "\n",
    "5. **Multiple Environments**:\n",
    "   - Two conda environments needed (`b2txt25` and `b2txt25_lm`)\n",
    "   - Different PyTorch versions (training vs. LM)\n",
    "   - Cannot mix environments\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start Recommendation\n",
    "\n",
    "**For Quick Testing:**\n",
    "1. Use **Kaggle Notebooks** (free, 30 hours/week)\n",
    "2. Upload project as dataset\n",
    "3. Run training in notebook\n",
    "4. Save checkpoints and download results\n",
    "\n",
    "**For Serious Training:**\n",
    "1. Use **AWS EC2 g4dn.xlarge** (~$1.75 per run)\n",
    "2. Follow AWS setup steps above\n",
    "3. Use `screen` for long-running training\n",
    "4. Download results when complete\n",
    "\n",
    "**For Cost-Conscious:**\n",
    "1. Use **Lambda Labs** or **Vast.ai** (~$1.23 per run)\n",
    "2. Similar setup to AWS\n",
    "3. Monitor usage carefully\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Questions to Clarify\n",
    "\n",
    "Before proceeding, please confirm:\n",
    "\n",
    "1. **What is your primary goal?**\n",
    "   - [ ] Just get baseline model running\n",
    "   - [ ] Compete in Kaggle competition\n",
    "   - [ ] Experiment with improvements\n",
    "   - [ ] Reproduce paper results\n",
    "\n",
    "2. **What resources do you have access to?**\n",
    "   - [ ] University compute cluster\n",
    "   - [ ] Local machine with NVIDIA GPU\n",
    "   - [ ] AWS/GCP/Azure account\n",
    "   - [ ] Budget for cloud computing ($1-5 per training run)\n",
    "   - [ ] Only free options\n",
    "\n",
    "3. **What is your timeline?**\n",
    "   - [ ] Need results ASAP\n",
    "   - [ ] Can wait for free resources\n",
    "   - [ ] Have weeks/months\n",
    "\n",
    "4. **What is your experience level?**\n",
    "   - [ ] Comfortable with Linux/cloud setup\n",
    "   - [ ] Prefer managed platforms (Colab/Kaggle)\n",
    "   - [ ] Need step-by-step guidance\n",
    "\n",
    "5. **Do you need the full pipeline?**\n",
    "   - [ ] Just training the RNN model\n",
    "   - [ ] Need language model evaluation too\n",
    "   - [ ] Need to generate submission file\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Next Steps\n",
    "\n",
    "Once you've chosen a platform:\n",
    "\n",
    "1. **Confirm platform choice**\n",
    "2. **Set up environment** (follow platform-specific steps)\n",
    "3. **Download and verify data**\n",
    "4. **Run training** (monitor closely first time)\n",
    "5. **Evaluate model** (validation set first)\n",
    "6. **Generate submission** (test set)\n",
    "7. **Submit to Kaggle**\n",
    "\n",
    "---\n",
    "\n",
    "**Document Version:** 1.0  \n",
    "**Last Updated:** 2025-01-XX  \n",
    "**Author:** Training Plan Generator\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
